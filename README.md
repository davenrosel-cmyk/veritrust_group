# VeriTrust Group - SRA Data Processing Pipeline

## ğŸ“‹ Project Overview

This repository contains a production-grade Tier-0 pipeline for transforming the Solicitors Regulation Authority (SRA) public dataset into canonical, normalized, SHACL-validated, JSON-LD datasets published nightly by VeriTrust Group Ltd.

### Key Features

- âœ… **Canonical Data**: Only source facts, no enrichments or derived concepts
- âœ… **Standard Normalization**: Clean and standardize names, addresses, and identifiers
- âœ… **SHACL Validation**: Data validation using SHACL Shapes
- âœ… **Schema.org Compliant**: JSON-LD output conforming to Schema.org standards
- âœ… **Digital Signatures**: Manifest generation with SHA-256 and optional RSA signature
- âœ… **AI-Ready**: ai.txt and robots.txt files for automated discovery

---

## ğŸ—ï¸ System Architecture

The pipeline executes in 6 main stages:

### Stage 1: Data Fetch
- Load SRA data from local file
- Save raw version for audit and tracking

### Stage 2: Normalization
- Normalize firm and office information:
  - Clean and standardize names
  - Build standard UK addresses
  - Extract and validate identifiers (sraId, officeId)
  - Normalize statuses and standard fields

### Stage 3: Validation
- Validate with SHACL Shapes (Tier-0)
- Check mandatory fields and basic data types
- Filter invalid records

### Stage 4: JSON-LD Build
- Convert to Schema.org compliant entities:
  - `schema:LegalService`
  - `schema:Organization`
  - `schema:PostalAddress`
  - `schema:ContactPoint`
  - VeriTrust Tier-0 ontology extensions

### Stage 5: Manifest & Signature
- Generate Manifest with canonical JSON
- Calculate SHA-256 for data integrity
- Optional RSA signature

### Stage 6: AI Discovery Files
- Generate `/ai.txt` listing public dataset URLs
- Generate `/robots.txt` to enable crawler access

---

## ğŸ“ Project Structure

```
venv/
â”œâ”€â”€ pipeline/                    # Main pipeline code
â”‚   â”œâ”€â”€ models/                  # Data models
â”‚   â”‚   â”œâ”€â”€ jsonld_models.py     # JSON-LD models
â”‚   â”‚   â””â”€â”€ raw_models.py        # Raw data models
â”‚   â”œâ”€â”€ constants.py             # Constants and configuration
â”‚   â”œâ”€â”€ fetch_sra.py             # SRA data fetching
â”‚   â”œâ”€â”€ normalize.py             # Data normalization
â”‚   â”œâ”€â”€ validate.py              # SHACL validation
â”‚   â”œâ”€â”€ jsonld_builder.py        # JSON-LD file generation
â”‚   â”œâ”€â”€ manifest_builder.py      # Manifest and signature generation
â”‚   â””â”€â”€ run_pipeline.py          # Main pipeline execution
â”‚
â”œâ”€â”€ input/                       # Input data
â”‚   â””â”€â”€ response.txt             # SRA data file
â”‚
â”œâ”€â”€ output/                      # Processed outputs
â”‚   â”œâ”€â”€ raw/                     # Raw data (for auditing)
â”‚   â”‚   â””â”€â”€ sra-YYYYMMDD.json
â”‚   â”œâ”€â”€ normalized/              # Normalized data
â”‚   â”‚   â”œâ”€â”€ firms.json
â”‚   â”‚   â”œâ”€â”€ offices.json
â”‚   â”‚   â””â”€â”€ manifest.jsonld
â”‚   â”œâ”€â”€ firms.jsonld             # JSON-LD output for firms
â”‚   â”œâ”€â”€ dataset.jsonld           # Complete JSON-LD output
â”‚   â””â”€â”€ manifest.jsonld          # Manifest with signature
â”‚
â”œâ”€â”€ ontology/                    # Ontology definitions
â”‚   â””â”€â”€ veritrust-min.ttl        # VeriTrust minimal ontology
â”‚
â”œâ”€â”€ shapes/                      # SHACL Shapes
â”‚   â””â”€â”€ tier0-shapes.ttl         # Tier-0 validation shapes
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸš€ Setup and Usage

### Prerequisites

- Python 3.11
- pip (Python package manager)

### Install Dependencies

```bash
pip install -r requirements.txt
```

Main dependencies:
- `pydantic`: Data validation and modeling
- `python-dateutil`: Date processing
- `cryptography`: Digital signatures and encryption

### Run Pipeline

```bash
python pipeline/run_pipeline.py
```

### Input Files

Before execution, ensure the SRA data file exists at:
```
input/response.txt
```

### Output Files

After successful execution, the following files are generated in the `output/` directory:

#### JSON-LD
- `firms.jsonld`: List of law firms in JSON-LD format
- `dataset.jsonld`: Complete dataset with all entities

#### Manifest
- `manifest.jsonld`: Manifest containing:
  - SHA-256 hash for data integrity
  - RSA signature (if enabled)
  - Publication metadata

#### Normalized Data
- `normalized/firms.json`: Normalized firms
- `normalized/offices.json`: Normalized offices

#### Raw Data (for auditing)
- `raw/sra-YYYYMMDD.json`: Raw version of fetched data

---

## ğŸ”§ Technical Details

### Data Models

#### Raw Models (`raw_models.py`)
- `RawFirmRecord`: Raw firm data structure from SRA
- `RawOfficeRecord`: Raw office data structure

#### Normalized Models
- `NormalizedFirm`: Normalized firm
- `NormalizedOffice`: Normalized office
- `NormalizedAddress`: Standardized address

#### JSON-LD Models (`jsonld_models.py`)
- Schema.org compliant entities
- VeriTrust ontology extensions

### Validation

The system uses SHACL (Shapes Constraint Language) for validation:
- Mandatory field checks
- Data type validation
- Basic integrity rules

Shapes are defined in `shapes/tier0-shapes.ttl`.

### Standards Used

- **JSON-LD**: Linked data format
- **Schema.org**: Standard vocabulary for web entities
- **SHACL**: Shapes Constraint Language for validation
- **RDF/Turtle**: For ontology and shapes definition

---

## ğŸ” Security and Integrity

### Hash and Signatures

- Each output file is hashed with SHA-256
- Manifest includes hash of all output files
- Optional RSA signature for authentication

### Auditing

- All raw data is stored in `output/raw/`
- Automatic file timestamping for change tracking
- Comprehensive logging for error tracking

---

## ğŸ“Š Deployment

### Railway Cron Job

The pipeline can be automatically executed on Railway:

**Schedule**: Every night at 02:00 UTC

```bash
python pipeline/run_pipeline.py
```

### Static File Hosting

- The `/output/` directory is served as the public dataset root
- `ai.txt` and `robots.txt` files enable automated discovery by search engines and AI

---

## ğŸ§ª Testing and Validation

### Data Integrity Checks

After pipeline execution, you can:

1. **Check Manifest File**: Verify hash and signature
2. **Validate JSON-LD**: Use JSON-LD validation tools
3. **SHACL Validation**: Run SHACL validation on output data

---

## ğŸ“ Logging

The system uses Python's standard `logging` module:

- Log level: `INFO`
- Logged information:
  - Number of records loaded
  - Normalization results
  - Validation results
  - Output file generation status

---

## ğŸ”„ Workflow

```
1. Fetch SRA Data
   â†“
2. Normalize (names, addresses, identifiers)
   â†“
3. Validate (SHACL)
   â†“
4. Build JSON-LD (Schema.org)
   â†“
5. Generate Manifest + Hash + Signature
   â†“
6. Generate AI Discovery Files
   â†“
âœ… Final output ready for publication
```

---

## ğŸ“š Resources and Documentation

- [Schema.org](https://schema.org/) - Standard vocabulary
- [JSON-LD](https://json-ld.org/) - Linked data format
- [SHACL](https://www.w3.org/TR/shacl/) - Shapes Constraint Language
- [SRA](https://www.sra.org.uk/) - Solicitors Regulation Authority

---

## ğŸ‘¥ Team and License

**Developer**: VeriTrust Group Ltd

**License**: Copyright Â© VeriTrust

---

## ğŸ“ Support

For questions and support, please contact the VeriTrust Group team.

---

## ğŸ”„ Versions and Updates

This pipeline is continuously updated to:
- Improve normalization quality
- Add new validations
- Optimize performance
- Maintain compatibility with latest Schema.org standards

---

**Last Updated**: 2025
